{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Release 1: https://press.un.org/en/2023/sgsm21982.doc.htm\n",
      "Press Release 2: https://press.un.org/en/2023/sgsm21980.doc.htm\n",
      "Press Release 3: https://press.un.org/en/2023/sgsm21978.doc.htm\n",
      "Press Release 4: https://press.un.org/en/2023/sgsm21947.doc.htm\n",
      "Press Release 5: https://press.un.org/en/2023/dsgsm1874.doc.htm\n",
      "Press Release 6: https://press.un.org/en/2023/sgsm21952.doc.htm\n",
      "Press Release 7: https://press.un.org/en/2023/sgsm21876.doc.htm\n",
      "Press Release 8: https://press.un.org/en/2023/sgsm21852.doc.htm\n",
      "Press Release 9: https://press.un.org/en/2023/sgsm21806.doc.htm\n",
      "Press Release 10: https://press.un.org/en/2023/dsgsm1848.doc.htm\n"
     ]
    }
   ],
   "source": [
    "# check if page release is there!!!\n",
    "def is_press_release(soup):\n",
    "    press_release_link = soup.find('a', href=\"/en/press-release\", hreflang=\"en\")\n",
    "    return press_release_link is not None\n",
    "\n",
    "# check for \"crisis\"\n",
    "def scrape(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    #get the entire text\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    #check for crisis\n",
    "    if is_press_release(soup):\n",
    "        if \"crisis\" in soup.get_text().lower():\n",
    "            return url\n",
    "\n",
    "    return None\n",
    "\n",
    "# Start with the seed URL\n",
    "seed_url = \"https://press.un.org/en\"\n",
    "base_url = \"https://press.un.org\"\n",
    "visited = set()\n",
    "to_visit = [seed_url]\n",
    "press_releases = []\n",
    "\n",
    "while to_visit and len(press_releases) < 10:\n",
    "    url = to_visit.pop(0)\n",
    "    if url in visited:\n",
    "        continue\n",
    "\n",
    "    visited.add(url)\n",
    "\n",
    "    # Extract links from the current page\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            next_url = link['href']\n",
    "            if next_url.startswith('/en'):\n",
    "                full_link = base_url + next_url\n",
    "                if full_link not in visited:\n",
    "                    to_visit.append(full_link)\n",
    "\n",
    "    press_release_url = scrape(url)\n",
    "    if press_release_url:\n",
    "        press_releases.append(press_release_url)\n",
    "\n",
    "# print\n",
    "for i, release_url in enumerate(press_releases[:10]):\n",
    "    print(f\"Press Release {i + 1}: {release_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Release 1: https://www.europarl.europa.eu/news/en/press-room/20230929IPR06132/nagorno-karabakh-meps-demand-review-of-eu-relations-with-azerbaijan\n",
      "Press Release 2: https://www.europarl.europa.eu/news/en/press-room/20221209IPR64426/eu-long-term-budget-needs-urgent-revision-to-cope-with-current-crises\n",
      "Press Release 3: https://www.europarl.europa.eu/news/en/press-room/20210304IPR99207/parliament-gives-green-light-for-new-eu4health-programme\n",
      "Press Release 4: https://www.europarl.europa.eu/news/en/press-room/20220909IPR40138/parliament-adopts-new-rules-on-adequate-minimum-wages-for-all-workers-in-the-eu\n",
      "Press Release 5: https://www.europarl.europa.eu/news/en/press-room/20230310IPR77232/minimum-income-schemes-increasing-support-accessibility-and-inclusion\n",
      "Press Release 6: https://www.europarl.europa.eu/news/en/press-room/20230210IPR74806/green-deal-industrial-plan-securing-the-eu-s-clean-tech-leadership\n",
      "Press Release 7: https://www.europarl.europa.eu/news/en/press-room/20230707IPR02421/parliament-adopts-new-rules-to-boost-energy-savings\n",
      "Press Release 8: https://www.europarl.europa.eu/news/en/press-room/20220321IPR25913/more-eu-action-needed-for-secure-food-supply\n",
      "Press Release 9: https://www.europarl.europa.eu/news/en/press-room/20221209IPR64427/holodomor-parliament-recognises-soviet-starvation-of-ukrainians-as-genocide\n",
      "Press Release 10: https://www.europarl.europa.eu/news/en/press-room/20210422IPR02615/civil-protection-faster-eu-response-to-large-scale-emergencies\n"
     ]
    }
   ],
   "source": [
    "#check for plenary session and press releases\n",
    "def is_plenary_session_press_release(soup):\n",
    "    return all(\n",
    "        soup.find('span', class_='ep_name', text=text)\n",
    "        for text in ['Plenary session', 'Press Releases']\n",
    "    )\n",
    "\n",
    "seed_url = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
    "url_num = 10  # max # of URLS\n",
    "urls = [seed_url]  # Queue of URLs to crawl\n",
    "old = set()       # Keep track of seen URLs\n",
    "pr_eps = []  # Store found press releases\n",
    "\n",
    "for url in urls:\n",
    "    if len(pr_eps) >= url_num:\n",
    "        break  \n",
    "\n",
    "    try:\n",
    "        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "    except Exception as ex:\n",
    "        continue  \n",
    "\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "\n",
    "    # Check for crisis\n",
    "    if is_plenary_session_press_release(soup) and \"crisis\" in soup.get_text().lower():\n",
    "        pr_eps.append(url)\n",
    "\n",
    "    # Put child URLs into the stack\n",
    "    child_urls = [\n",
    "        urllib.parse.urljoin(seed_url, tag['href'])\n",
    "        for tag in soup.find_all('a', href=True)\n",
    "    ]\n",
    "\n",
    "    # Filter and update the URLs and seen set\n",
    "    for child_url in child_urls:\n",
    "        if seed_url in child_url and child_url not in old and child_url not in urls:\n",
    "            urls.append(child_url)\n",
    "            old.add(child_url)\n",
    "\n",
    "# Print the extracted press releases\n",
    "for idx, release_url in enumerate(pr_eps):\n",
    "    print(f\"Press Release {idx + 1}: {release_url}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
